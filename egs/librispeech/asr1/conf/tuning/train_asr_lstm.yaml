batch_type: numel
patience: 100
# The initialization method for model parameters
init: chainer # Don't change this initialization
best_model_criterion:
-   - valid
    - acc
    - max
keep_nbest_models: 1

encoder: vgg_rnn
encoder_conf:
    rnn_type: lstm
    bidirectional: true
    use_projection: true
    num_layers: 4
    hidden_size: 1024
    output_size: 1024

decoder: rnn
decoder_conf:
    rnn_type: lstm
    num_layers: 2
    hidden_size: 1024
    sampling_probability: 0.0
    att_conf:
        atype: location
        adim: 1024
        awin: 5
        aheads: 2
        aconv_chans: 10
        aconv_filts: 100

optim: adadelta
optim_conf:
    lr: 1.0
    rho: 0.95
    eps: 1.0e-08
    weight_decay: 0

specaug: specaug
specaug_conf:
    apply_time_warp: true
    time_warp_window: 5
    time_warp_mode: bicubic
    apply_freq_mask: true
    freq_mask_width_range:
    - 0
    - 30
    num_freq_mask: 2
    apply_time_mask: true
    time_mask_width_range:
    - 0
    - 40
    num_time_mask: 2

# Custom Settings
# model_conf:
#     ctc_weight: 0.5
#     lsm_weight: 0.1
#     length_normalized_loss: false

# # training the clean labels
# stage: 1
# max_epoch: 80
# batch_bins: 14000000
# accum_grad: 1

# # We should run this stage with ngpu == 1
# # batch_bins: 100000000 -> 4gpu settings
# stage: 2
# max_epoch: 1
# batch_bins: 26000000 
# init_param: 
#     - exp/asr_train_asr_lstm_raw_en_bpe5000_semi_stage1/79epoch.pth:encoder:encoder
#     - exp/asr_train_asr_lstm_raw_en_bpe5000_semi_stage1/79epoch.pth:decoder:decoder

model_conf:
    ctc_weight: 0.5
    lsm_weight: 0.1
    length_normalized_loss: false

stage: 3
max_epoch: 180
batch_bins: 52000000
accum_grad: 1
init_param: 
    - exp/asr_train_asr_lstm_raw_en_bpe5000_semi_stage2/1epoch.pth:stat:stat
    - exp/asr_train_asr_lstm_raw_en_bpe5000_semi_stage1/79epoch.pth:encoder:meta_encoder
    - exp/asr_train_asr_lstm_raw_en_bpe5000_semi_stage1/79epoch.pth:decoder:meta_decoder
    - exp/asr_train_asr_lstm_raw_en_bpe5000_semi_stage1/79epoch.pth:encoder:encoder
    - exp/asr_train_asr_lstm_raw_en_bpe5000_semi_stage1/79epoch.pth:decoder:decoder
unused_parameters: true
freeze_param:
    - meta_encoder
    - meta_decoder

# lm: transformer
# lm_conf:
#     pos_enc: null
#     embed_unit: 128
#     att_unit: 512
#     head: 8
#     unit: 2048
#     layer: 16
#     dropout_rate: 0.0
# init_param: 
#     - exp/lm_train_lm_transformer2_en_bpe5000/valid.loss.ave_10best.pth:lm:lm
# freeze_param:
#     - lm